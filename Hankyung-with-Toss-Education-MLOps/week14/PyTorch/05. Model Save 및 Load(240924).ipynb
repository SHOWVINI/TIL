{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FashionMNIST Dataset & DataLoader 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FashionMNIST 데이터세트 다운로드\n",
    "\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=ToTensor() \n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataloader  = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        # nn.Module 생성\n",
    "        super(NeuralNetwork, self).__init__() \n",
    "        \n",
    "        # 레이어 정의\n",
    "        self.flatten = nn.Flatten() # 평탄화 레이어 정의\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 입력되는 x의 모양이 (64, 1, 28, 28) -> (N, C, H, W)\n",
    "            # 입력 이미지에 대한 평탄화가 필요\n",
    "        x = self.flatten(x) # flatten 레이어를 지나게 되면 (64, 784)\n",
    "        y = self.linear_relu_stack(x)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# 파이토치를 이용해 모델 객체를 만들고 나서 어떤 장치(device) 환경에서 훈련이나 추론을 수행할지 결정지어주기\n",
    "\n",
    "import torch\n",
    "\n",
    "# MPS 지원 여부 확인\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'  # Apple Silicon에서 MPS 사용\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'cuda'  # CUDA 사용 가능 시\n",
    "else:\n",
    "    device = 'cpu'  # 그 외에는 CPU 사용\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss() # 이미 여기에 소프트맥스 함수가 포함되어 있다.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 과정\n",
    "    # 데이터 로딩 -> 예측 -> Loss 계산 -> 미분(backward) -> 최적화\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    \n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    # 모델을 훈련 모드로 설정\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        \n",
    "        # 데이터 로더에 들어있던 텐서들을 모델과 같은 위치(모델이 GPU니까 데이터도 GPU)로  옮기기\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # 예측(forward)\n",
    "        pred = model(X) # softmax가 적용 안되어있음\n",
    "\n",
    "        # Loss 계산\n",
    "        loss = loss_fn(pred, y) # loss function 내에서 softmax가 적용 된 다음 y에 대한 loss를 구한다.\n",
    "\n",
    "        # 역전파 수행\n",
    "        optimizer.zero_grad() # 이전 배치에 남아있는 기울기를 제거\n",
    "        loss.backward() # 오차 역전파\n",
    "        optimizer.step()\n",
    "\n",
    "        # 배치가 100번 돌 때마다 화면에 출력\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"Train Loss : {loss:>7f} [ {current:>5d} / {size:>5d} ]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    # loss는 배치 별로 계산, correct는 전체 데이터 세트에 대한 평균 정확도\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # 모델을 추론 모드로 바꿔준다.\n",
    "    model.eval()\n",
    "\n",
    "    # 추론 과정에서는 기울기를 구할 필요가 없음. 따라서 모든 파라미터(model.parameters())의 required_grad=False\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            pred = model(X)\n",
    "\n",
    "            # test_loss를 배치마다 구해서 더해주기\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            # 10개의 예측 값중 가장 큰 곳의 인덱스를 argmax로 찾고, 타겟(y)와 일치하는지 확인\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    # 배치 개수 구하기\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    # Loss 평균 구하기\n",
    "    test_loss /= num_batches\n",
    "\n",
    "    # Accuracy 구하기\n",
    "    correct /= size\n",
    "\n",
    "    print(f\"Test Error : \\n Accuracy : {(100*correct):>0.1f}%, Avg Loss : {test_loss:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 1\n",
      "-------------------------------------------\n",
      "Train Loss : 0.429134 [     0 / 60000 ]\n",
      "Train Loss : 0.532000 [  6400 / 60000 ]\n",
      "Train Loss : 0.439087 [ 12800 / 60000 ]\n",
      "Train Loss : 0.376231 [ 19200 / 60000 ]\n",
      "Train Loss : 0.217501 [ 25600 / 60000 ]\n",
      "Train Loss : 0.502792 [ 32000 / 60000 ]\n",
      "Train Loss : 0.334343 [ 38400 / 60000 ]\n",
      "Train Loss : 0.309730 [ 44800 / 60000 ]\n",
      "Train Loss : 0.400801 [ 51200 / 60000 ]\n",
      "Train Loss : 0.333813 [ 57600 / 60000 ]\n",
      "\n",
      "\n",
      "Epochs 2\n",
      "-------------------------------------------\n",
      "Train Loss : 0.307991 [     0 / 60000 ]\n",
      "Train Loss : 0.298345 [  6400 / 60000 ]\n",
      "Train Loss : 0.414203 [ 12800 / 60000 ]\n",
      "Train Loss : 0.280404 [ 19200 / 60000 ]\n",
      "Train Loss : 0.341085 [ 25600 / 60000 ]\n",
      "Train Loss : 0.444079 [ 32000 / 60000 ]\n",
      "Train Loss : 0.405520 [ 38400 / 60000 ]\n",
      "Train Loss : 0.426117 [ 44800 / 60000 ]\n",
      "Train Loss : 0.363793 [ 51200 / 60000 ]\n",
      "Train Loss : 0.382632 [ 57600 / 60000 ]\n",
      "\n",
      "\n",
      "Epochs 3\n",
      "-------------------------------------------\n",
      "Train Loss : 0.287062 [     0 / 60000 ]\n",
      "Train Loss : 0.409481 [  6400 / 60000 ]\n",
      "Train Loss : 0.311404 [ 12800 / 60000 ]\n",
      "Train Loss : 0.325068 [ 19200 / 60000 ]\n",
      "Train Loss : 0.324831 [ 25600 / 60000 ]\n",
      "Train Loss : 0.361390 [ 32000 / 60000 ]\n",
      "Train Loss : 0.483787 [ 38400 / 60000 ]\n",
      "Train Loss : 0.240304 [ 44800 / 60000 ]\n",
      "Train Loss : 0.392000 [ 51200 / 60000 ]\n",
      "Train Loss : 0.286920 [ 57600 / 60000 ]\n",
      "\n",
      "\n",
      "Epochs 4\n",
      "-------------------------------------------\n",
      "Train Loss : 0.299777 [     0 / 60000 ]\n",
      "Train Loss : 0.434809 [  6400 / 60000 ]\n",
      "Train Loss : 0.374554 [ 12800 / 60000 ]\n",
      "Train Loss : 0.204185 [ 19200 / 60000 ]\n",
      "Train Loss : 0.259185 [ 25600 / 60000 ]\n",
      "Train Loss : 0.350420 [ 32000 / 60000 ]\n",
      "Train Loss : 0.221379 [ 38400 / 60000 ]\n",
      "Train Loss : 0.479031 [ 44800 / 60000 ]\n",
      "Train Loss : 0.358035 [ 51200 / 60000 ]\n",
      "Train Loss : 0.331199 [ 57600 / 60000 ]\n",
      "\n",
      "\n",
      "Epochs 5\n",
      "-------------------------------------------\n",
      "Train Loss : 0.234537 [     0 / 60000 ]\n",
      "Train Loss : 0.315328 [  6400 / 60000 ]\n",
      "Train Loss : 0.172622 [ 12800 / 60000 ]\n",
      "Train Loss : 0.419927 [ 19200 / 60000 ]\n",
      "Train Loss : 0.345190 [ 25600 / 60000 ]\n",
      "Train Loss : 0.216403 [ 32000 / 60000 ]\n",
      "Train Loss : 0.276604 [ 38400 / 60000 ]\n",
      "Train Loss : 0.660069 [ 44800 / 60000 ]\n",
      "Train Loss : 0.421154 [ 51200 / 60000 ]\n",
      "Train Loss : 0.303364 [ 57600 / 60000 ]\n",
      "\n",
      "\n",
      "Epochs 6\n",
      "-------------------------------------------\n",
      "Train Loss : 0.274039 [     0 / 60000 ]\n",
      "Train Loss : 0.234059 [  6400 / 60000 ]\n",
      "Train Loss : 0.319232 [ 12800 / 60000 ]\n",
      "Train Loss : 0.332511 [ 19200 / 60000 ]\n",
      "Train Loss : 0.317502 [ 25600 / 60000 ]\n",
      "Train Loss : 0.484945 [ 32000 / 60000 ]\n",
      "Train Loss : 0.354455 [ 38400 / 60000 ]\n",
      "Train Loss : 0.324752 [ 44800 / 60000 ]\n",
      "Train Loss : 0.352924 [ 51200 / 60000 ]\n",
      "Train Loss : 0.157183 [ 57600 / 60000 ]\n",
      "\n",
      "\n",
      "Epochs 7\n",
      "-------------------------------------------\n",
      "Train Loss : 0.268597 [     0 / 60000 ]\n",
      "Train Loss : 0.390692 [  6400 / 60000 ]\n",
      "Train Loss : 0.346875 [ 12800 / 60000 ]\n",
      "Train Loss : 0.268742 [ 19200 / 60000 ]\n",
      "Train Loss : 0.309277 [ 25600 / 60000 ]\n",
      "Train Loss : 0.266495 [ 32000 / 60000 ]\n",
      "Train Loss : 0.486557 [ 38400 / 60000 ]\n",
      "Train Loss : 0.291480 [ 44800 / 60000 ]\n",
      "Train Loss : 0.473784 [ 51200 / 60000 ]\n",
      "Train Loss : 0.332894 [ 57600 / 60000 ]\n",
      "\n",
      "\n",
      "Epochs 8\n",
      "-------------------------------------------\n",
      "Train Loss : 0.343701 [     0 / 60000 ]\n",
      "Train Loss : 0.292955 [  6400 / 60000 ]\n",
      "Train Loss : 0.377847 [ 12800 / 60000 ]\n",
      "Train Loss : 0.235567 [ 19200 / 60000 ]\n",
      "Train Loss : 0.263397 [ 25600 / 60000 ]\n",
      "Train Loss : 0.351627 [ 32000 / 60000 ]\n",
      "Train Loss : 0.481215 [ 38400 / 60000 ]\n",
      "Train Loss : 0.254478 [ 44800 / 60000 ]\n",
      "Train Loss : 0.381306 [ 51200 / 60000 ]\n",
      "Train Loss : 0.263842 [ 57600 / 60000 ]\n",
      "\n",
      "\n",
      "Epochs 9\n",
      "-------------------------------------------\n",
      "Train Loss : 0.315881 [     0 / 60000 ]\n",
      "Train Loss : 0.281583 [  6400 / 60000 ]\n",
      "Train Loss : 0.465344 [ 12800 / 60000 ]\n",
      "Train Loss : 0.414253 [ 19200 / 60000 ]\n",
      "Train Loss : 0.299016 [ 25600 / 60000 ]\n",
      "Train Loss : 0.351012 [ 32000 / 60000 ]\n",
      "Train Loss : 0.320685 [ 38400 / 60000 ]\n",
      "Train Loss : 0.367400 [ 44800 / 60000 ]\n",
      "Train Loss : 0.430192 [ 51200 / 60000 ]\n",
      "Train Loss : 0.262528 [ 57600 / 60000 ]\n",
      "\n",
      "\n",
      "Epochs 10\n",
      "-------------------------------------------\n",
      "Train Loss : 0.320863 [     0 / 60000 ]\n",
      "Train Loss : 0.265825 [  6400 / 60000 ]\n",
      "Train Loss : 0.372121 [ 12800 / 60000 ]\n",
      "Train Loss : 0.514210 [ 19200 / 60000 ]\n",
      "Train Loss : 0.336304 [ 25600 / 60000 ]\n",
      "Train Loss : 0.277264 [ 32000 / 60000 ]\n",
      "Train Loss : 0.365145 [ 38400 / 60000 ]\n",
      "Train Loss : 0.354473 [ 44800 / 60000 ]\n",
      "Train Loss : 0.425459 [ 51200 / 60000 ]\n",
      "Train Loss : 0.322764 [ 57600 / 60000 ]\n",
      "\n",
      "\n",
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for i in range(epochs):\n",
    "    print(f\"Epochs {i + 1}\\n-------------------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(\"Done!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기까지가 01~~~~04. PyTorch Model Training | Validation까지 (240923)에 학습한 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련된 모델의 가중치를 저장 / 불러오기\n",
    "불러올 곳에서 **모델의 구조를 알고 있는 경우** 가중치만 저장하면 적은 용량으로 저장하고 불러오는 것이 가능합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.state_dict() : 모델 내에 있는 레이어 별 가중치를 들고 있는 딕셔너리\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('linear_relu_stack.0.weight',\n",
       "              tensor([[-0.0110,  0.0362,  0.0385,  ..., -0.0009, -0.0659, -0.0108],\n",
       "                      [ 0.0617, -0.0028,  0.0633,  ..., -0.0469, -0.0738, -0.0460],\n",
       "                      [ 0.0419,  0.0107,  0.0445,  ...,  0.0703,  0.1044,  0.0528],\n",
       "                      ...,\n",
       "                      [ 0.0210,  0.0938,  0.0686,  ..., -0.0150, -0.0394,  0.0397],\n",
       "                      [ 0.0263, -0.0276, -0.0540,  ..., -0.0514,  0.0012, -0.0247],\n",
       "                      [-0.0313,  0.0123,  0.0644,  ..., -0.0210, -0.0018,  0.0526]],\n",
       "                     device='mps:0')),\n",
       "             ('linear_relu_stack.0.bias',\n",
       "              tensor([ 2.4537e-01,  3.6434e-02, -8.9375e-02,  1.0470e-01,  3.5253e-01,\n",
       "                       2.1932e-01,  5.5733e-02,  1.2115e-02, -2.2888e-01, -4.3831e-02,\n",
       "                      -4.2541e-02,  1.3912e-01, -6.2271e-02, -8.9916e-03,  1.6770e-01,\n",
       "                       5.2324e-02,  1.3337e-01, -9.7619e-03,  1.9156e-01, -1.0552e-01,\n",
       "                      -3.8510e-03,  1.4487e-03,  1.6698e-01,  7.2722e-02,  4.5421e-02,\n",
       "                       1.9214e-01, -3.5897e-02, -1.7424e-02, -1.3406e-01,  2.2739e-02,\n",
       "                      -2.6764e-01,  2.7969e-01,  1.8169e-01, -1.3870e-01,  2.3887e-01,\n",
       "                      -5.0431e-02,  4.3233e-02, -9.5974e-02,  1.7917e-01, -1.4205e-01,\n",
       "                      -6.7546e-02, -1.1675e-01,  3.1261e-01,  3.9473e-01,  6.0612e-02,\n",
       "                      -1.1906e-01,  5.8582e-02, -2.7824e-02,  1.3294e-01, -9.2700e-02,\n",
       "                       1.7832e-01, -2.7697e-01,  4.4363e-02,  9.6151e-02,  9.2944e-02,\n",
       "                       1.9847e-01,  1.1958e-01, -1.0202e-02,  2.5982e-01, -8.6842e-03,\n",
       "                       1.7866e-01,  1.2618e-01,  1.3940e-01,  1.3077e-01,  1.8725e-01,\n",
       "                      -1.5088e-01,  2.2394e-01,  2.6504e-01,  2.4343e-01,  9.9320e-02,\n",
       "                      -1.9941e-01, -8.9794e-02,  1.4783e-01,  1.7217e-01,  2.0989e-01,\n",
       "                      -1.3773e-01,  2.3440e-01,  1.9156e-01,  7.7728e-02,  1.8127e-01,\n",
       "                       3.1550e-01,  2.4398e-02,  2.8541e-02, -1.3647e-01,  1.7018e-01,\n",
       "                       1.8673e-01,  1.6724e-01,  8.8388e-02, -8.9093e-02, -1.0589e-01,\n",
       "                      -1.4197e-01,  2.6684e-01, -1.2930e-01, -2.3067e-02, -2.5500e-01,\n",
       "                      -1.8307e-01,  1.2172e-01,  2.1024e-04,  2.3608e-02,  2.4985e-01,\n",
       "                       1.4729e-01,  1.1913e-01, -1.9144e-02,  1.6744e-01, -1.0270e-01,\n",
       "                      -2.0503e-01,  2.2919e-01,  1.6040e-01,  2.4124e-01, -5.4176e-02,\n",
       "                       1.1078e-03,  3.1553e-02,  2.3517e-01,  1.8663e-01, -6.0279e-02,\n",
       "                       9.3539e-02,  6.8913e-02,  1.0335e-01, -5.9990e-03, -1.0769e-01,\n",
       "                       2.0767e-01,  1.2660e-01,  5.6300e-02, -1.1643e-01,  1.1122e-01,\n",
       "                       1.9595e-01,  3.8353e-01,  1.2236e-01], device='mps:0')),\n",
       "             ('linear_relu_stack.2.weight',\n",
       "              tensor([[ 0.2396,  0.0751, -0.1072,  ..., -0.0040, -0.1851, -0.2583],\n",
       "                      [-0.0206, -0.0807, -0.1170,  ...,  0.1239, -0.7390, -0.2914],\n",
       "                      [-0.0812, -0.0896,  0.0218,  ...,  0.1162, -0.3198, -0.1690],\n",
       "                      ...,\n",
       "                      [-0.0280,  0.1487,  0.0217,  ...,  0.1694, -0.0056, -0.3548],\n",
       "                      [-0.3059,  0.0389, -0.1119,  ..., -0.0733, -0.2018, -0.0566],\n",
       "                      [-0.3472, -0.0815,  0.1201,  ..., -0.0511, -0.1137,  0.0955]],\n",
       "                     device='mps:0')),\n",
       "             ('linear_relu_stack.2.bias',\n",
       "              tensor([ 0.0184, -0.2483,  0.1141,  0.1552, -0.2264,  0.1814,  0.0541,  0.0150,\n",
       "                      -0.1047, -0.3074], device='mps:0'))])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 저장된 가중치 파일(pth) 불러오기\n",
    "\n",
    "# window\n",
    "# model2 = NeuralNetwork().cuda() # .to('cuda')\n",
    "# print(model2)\n",
    "\n",
    "# mac (device = torch.device(\"mps\"))\n",
    "model2 = NeuralNetwork().to(device) # 모델을 MPS 또는 CPU로 이동\n",
    "print(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error : \n",
      " Accuracy : 10.6%, Avg Loss : 2.313483\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 훈련되지 않은 모델로 검증하면 당연히 성능이 좋지 않다.\n",
    "test_loop(test_dataloader, model2, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zk/p59k4t_16mlgf68b5hxclv6m0000gn/T/ipykernel_31928/3017254588.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model2.load_state_dict(torch.load('model_weights.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error : \n",
      " Accuracy : 87.2%, Avg Loss : 0.368632\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 이전에 훈련된 가중치를 불러와서 model2에서 로드\n",
    "model2.load_state_dict(torch.load('model_weights.pth'))\n",
    "test_loop(test_dataloader, model2, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 훈련된 모델 자체를 저장 / 불러오기\n",
    "모델의 구조를 모르는 경우 사용할 수 있는 대표적인 방법으로서 가중치만 저장한 경우보다 파일의 크기는 크지만 구조를 몰라도 모델을 사용할 수 있다는 장점이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 khb43  staff  410496 Sep 24 10:39 model.pth\n",
      "-rw-r--r--   1 khb43  staff  409344 Sep 24 10:36 model_weights.pth\n"
     ]
    }
   ],
   "source": [
    "!ls -al | grep pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zk/p59k4t_16mlgf68b5hxclv6m0000gn/T/ipykernel_31928/466480325.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model3 = torch.load('model.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error : \n",
      " Accuracy : 87.2%, Avg Loss : 0.368632\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model3 = torch.load('model.pth')\n",
    "test_loop(test_dataloader, model3, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
